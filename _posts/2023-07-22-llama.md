
# The model structure of LLaMA 2

Recently Meta makes the LLaMA 2 model public available. Here I will run the model and see the structure of the model.

To run the model we need to first install torch and torchrun. While I encounter error "failed to load torch._c...", the following code does not produce error:
```
python -m torch.distributed.run .py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model     --max_seq_len 512 --max_batch_
size 4
```
Now let's print the model by adding 
```
print(generator.model)
```
to the "example_chat_completion.py" file. The result is

```
Transformer(
  (tok_embeddings): ParallelEmbedding()
  (layers): ModuleList(
    (0-31): 32 x TransformerBlock(
      (attention): Attention(
        (wq): ColumnParallelLinear()
        (wk): ColumnParallelLinear()
        (wv): ColumnParallelLinear()
        (wo): RowParallelLinear()
      )
      (feed_forward): FeedForward(
        (w1): ColumnParallelLinear()
        (w2): RowParallelLinear()
        (w3): ColumnParallelLinear()
      )
      (attention_norm): RMSNorm()
      (ffn_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): ColumnParallelLinear()
)
```
Next let's use the summary function from torchinfo package to show the details.
```
from torchinfo import summary
print(summary(generator.model))
```
Thre result is 
```
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
Transformer                                        --
├─ParallelEmbedding: 1-1                           131,072,000
├─ModuleList: 1-2                                  --
│    └─TransformerBlock: 2-1                       --
│    │    └─Attention: 3-1                         67,108,864
│    │    └─FeedForward: 3-2                       135,266,304
│    │    └─RMSNorm: 3-3                           4,096
│    │    └─RMSNorm: 3-4                           4,096
│    └─TransformerBlock: 2-2                       --
│    │    └─Attention: 3-5                         67,108,864
│    │    └─FeedForward: 3-6                       135,266,304
│    │    └─RMSNorm: 3-7                           4,096
│    │    └─RMSNorm: 3-8                           4,096
│    └─TransformerBlock: 2-3                       --
│    │    └─Attention: 3-9                         67,108,864
│    │    └─FeedForward: 3-10                      135,266,304
│    │    └─RMSNorm: 3-11                          4,096
│    │    └─RMSNorm: 3-12                          4,096
│    └─TransformerBlock: 2-4                       --
│    │    └─Attention: 3-13                        67,108,864
│    │    └─FeedForward: 3-14                      135,266,304
│    │    └─RMSNorm: 3-15                          4,096
│    │    └─RMSNorm: 3-16                          4,096
│    └─TransformerBlock: 2-5                       --
│    │    └─Attention: 3-17                        67,108,864
│    │    └─FeedForward: 3-18                      135,266,304
│    │    └─RMSNorm: 3-19                          4,096
│    │    └─RMSNorm: 3-20                          4,096
│    └─TransformerBlock: 2-6                       --
│    │    └─Attention: 3-21                        67,108,864
│    │    └─FeedForward: 3-22                      135,266,304
│    │    └─RMSNorm: 3-23                          4,096
│    │    └─RMSNorm: 3-24                          4,096
│    └─TransformerBlock: 2-7                       --
│    │    └─Attention: 3-25                        67,108,864
│    │    └─FeedForward: 3-26                      135,266,304
│    │    └─RMSNorm: 3-27                          4,096
│    │    └─RMSNorm: 3-28                          4,096
│    └─TransformerBlock: 2-8                       --
│    │    └─Attention: 3-29                        67,108,864
│    │    └─FeedForward: 3-30                      135,266,304
│    │    └─RMSNorm: 3-31                          4,096
│    │    └─RMSNorm: 3-32                          4,096
│    └─TransformerBlock: 2-9                       --
│    │    └─Attention: 3-33                        67,108,864
│    │    └─FeedForward: 3-34                      135,266,304
│    │    └─RMSNorm: 3-35                          4,096
│    │    └─RMSNorm: 3-36                          4,096
│    └─TransformerBlock: 2-10                      --
│    │    └─Attention: 3-37                        67,108,864
│    │    └─FeedForward: 3-38                      135,266,304
│    │    └─RMSNorm: 3-39                          4,096
│    │    └─RMSNorm: 3-40                          4,096
│    └─TransformerBlock: 2-11                      --
│    │    └─Attention: 3-41                        67,108,864
│    │    └─FeedForward: 3-42                      135,266,304
│    │    └─RMSNorm: 3-43                          4,096
│    │    └─RMSNorm: 3-44                          4,096
│    └─TransformerBlock: 2-12                      --
│    │    └─Attention: 3-45                        67,108,864
│    │    └─FeedForward: 3-46                      135,266,304
│    │    └─RMSNorm: 3-47                          4,096
│    │    └─RMSNorm: 3-48                          4,096
│    └─TransformerBlock: 2-13                      --
│    │    └─Attention: 3-49                        67,108,864
│    │    └─FeedForward: 3-50                      135,266,304
│    │    └─RMSNorm: 3-51                          4,096
│    │    └─RMSNorm: 3-52                          4,096
│    └─TransformerBlock: 2-14                      --
│    │    └─Attention: 3-53                        67,108,864
│    │    └─FeedForward: 3-54                      135,266,304
│    │    └─RMSNorm: 3-55                          4,096
│    │    └─RMSNorm: 3-56                          4,096
│    └─TransformerBlock: 2-15                      --
│    │    └─Attention: 3-57                        67,108,864
│    │    └─FeedForward: 3-58                      135,266,304
│    │    └─RMSNorm: 3-59                          4,096
│    │    └─RMSNorm: 3-60                          4,096
│    └─TransformerBlock: 2-16                      --
│    │    └─Attention: 3-61                        67,108,864
│    │    └─FeedForward: 3-62                      135,266,304
│    │    └─RMSNorm: 3-63                          4,096
│    │    └─RMSNorm: 3-64                          4,096
│    └─TransformerBlock: 2-17                      --
│    │    └─Attention: 3-65                        67,108,864
│    │    └─FeedForward: 3-66                      135,266,304
│    │    └─RMSNorm: 3-67                          4,096
│    │    └─RMSNorm: 3-68                          4,096
│    └─TransformerBlock: 2-18                      --
│    │    └─Attention: 3-69                        67,108,864
│    │    └─FeedForward: 3-70                      135,266,304
│    │    └─RMSNorm: 3-71                          4,096
│    │    └─RMSNorm: 3-72                          4,096
│    └─TransformerBlock: 2-19                      --
│    │    └─Attention: 3-73                        67,108,864
│    │    └─FeedForward: 3-74                      135,266,304
│    │    └─RMSNorm: 3-75                          4,096
│    │    └─RMSNorm: 3-76                          4,096
│    └─TransformerBlock: 2-20                      --
│    │    └─Attention: 3-77                        67,108,864
│    │    └─FeedForward: 3-78                      135,266,304
│    │    └─RMSNorm: 3-79                          4,096
│    │    └─RMSNorm: 3-80                          4,096
│    └─TransformerBlock: 2-21                      --
│    │    └─Attention: 3-81                        67,108,864
│    │    └─FeedForward: 3-82                      135,266,304
│    │    └─RMSNorm: 3-83                          4,096
│    │    └─RMSNorm: 3-84                          4,096
│    └─TransformerBlock: 2-22                      --
│    │    └─Attention: 3-85                        67,108,864
│    │    └─FeedForward: 3-86                      135,266,304
│    │    └─RMSNorm: 3-87                          4,096
│    │    └─RMSNorm: 3-88                          4,096
│    └─TransformerBlock: 2-23                      --
│    │    └─Attention: 3-89                        67,108,864
│    │    └─FeedForward: 3-90                      135,266,304
│    │    └─RMSNorm: 3-91                          4,096
│    │    └─RMSNorm: 3-92                          4,096
│    └─TransformerBlock: 2-24                      --
│    │    └─Attention: 3-93                        67,108,864
│    │    └─FeedForward: 3-94                      135,266,304
│    │    └─RMSNorm: 3-95                          4,096
│    │    └─RMSNorm: 3-96                          4,096
│    └─TransformerBlock: 2-25                      --
│    │    └─Attention: 3-97                        67,108,864
│    │    └─FeedForward: 3-98                      135,266,304
│    │    └─RMSNorm: 3-99                          4,096
│    │    └─RMSNorm: 3-100                         4,096
│    └─TransformerBlock: 2-26                      --
│    │    └─Attention: 3-101                       67,108,864
│    │    └─FeedForward: 3-102                     135,266,304
│    │    └─RMSNorm: 3-103                         4,096
│    │    └─RMSNorm: 3-104                         4,096
│    └─TransformerBlock: 2-27                      --
│    │    └─Attention: 3-105                       67,108,864
│    │    └─FeedForward: 3-106                     135,266,304
│    │    └─RMSNorm: 3-107                         4,096
│    │    └─RMSNorm: 3-108                         4,096
│    └─TransformerBlock: 2-28                      --
│    │    └─Attention: 3-109                       67,108,864
│    │    └─FeedForward: 3-110                     135,266,304
│    │    └─RMSNorm: 3-111                         4,096
│    │    └─RMSNorm: 3-112                         4,096
│    └─TransformerBlock: 2-29                      --
│    │    └─Attention: 3-113                       67,108,864
│    │    └─FeedForward: 3-114                     135,266,304
│    │    └─RMSNorm: 3-115                         4,096
│    │    └─RMSNorm: 3-116                         4,096
│    └─TransformerBlock: 2-30                      --
│    │    └─Attention: 3-117                       67,108,864
│    │    └─FeedForward: 3-118                     135,266,304
│    │    └─RMSNorm: 3-119                         4,096
│    │    └─RMSNorm: 3-120                         4,096
│    └─TransformerBlock: 2-31                      --
│    │    └─Attention: 3-121                       67,108,864
│    │    └─FeedForward: 3-122                     135,266,304
│    │    └─RMSNorm: 3-123                         4,096
│    │    └─RMSNorm: 3-124                         4,096
│    └─TransformerBlock: 2-32                      --
│    │    └─Attention: 3-125                       67,108,864
│    │    └─FeedForward: 3-126                     135,266,304
│    │    └─RMSNorm: 3-127                         4,096
│    │    └─RMSNorm: 3-128                         4,096
├─RMSNorm: 1-3                                     4,096
├─ColumnParallelLinear: 1-4                        131,072,000
===========================================================================
Total params: 6,738,415,616
Trainable params: 6,738,415,616
Non-trainable params: 0
===========================================================================
```
So the 7B model has  6,738,415,616 parameters, i.e. ~ 7 billion parameters. 

The model looks quite simply, just 32 transformer blocks. Before we analyze the transformer, let's first take a look at the token embedding.

## Token embedding

### Tokenization
Tokenization is the process of chopping the sentence into parts. For example:

You're right. --> you ' are right -> 123, 234, 345, 456

LLaMA uses the "SentencePieceProcessor" package from google. To tokenize a sentence, we can use 
```
from llama.tokenizer import Tokenizer
tokenizer = Tokenizer(model_path='llama_tokenizer.model')
prompt = "you're right"
print(tokenizer.encode(prompt, None, None))
```
We can get the output [366, 29915, 276, 1492] which is a vector.

### Embedding
To map the tokenization result into matrices as input to the model, we need pass the result over an embedding layer.
In the "model.py" file, we can see the definition of the token embedding layer:
```
self.tok_embeddings = ParallelEmbedding(
     params.vocab_size, params.dim, init_method=lambda x: x
)
```
The "ParallelEmbedding" function is the corresponding parallel version of the "torch.nn.embedding" function implemented in "fairscale package".
Let's define the single layer and see its function.
```
tok_embedding = torch.nn.Embedding(num_embeddings = 32000, embedding_dim = 4096)
```
Embedding can be seen as a linear layer that makes the vector of words into matrice with continuous numbers. The number of embeddings is the size of the dictionary of the embeddings and the dimension is the size of each embedding vector.  

For example, if the number of embedding is taken to be 3 and the dimension is taken to be 2.
```
import torch.nn.functional as F
input = torch.tensor([0,2,1])
print(embedding_matrix = torch.rand(3, 3))
print(F.embedding(input, embedding_matrix))
```
the result is 
```
tensor([[0.8340, 0.8084],
        [0.6653, 0.7020],
        [0.9347, 0.2200]])
tensor([[0.8340, 0.8084],
        [0.9347, 0.2200],
        [0.6653, 0.7020]])
```

<!--- ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=None, vocab_size=32000, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-06, max_batch_size=4, max_seq_len=512) --->

## The transformer
The whole structure of the LLaMA2 model is 

embedding layer -> TransformerBlock *32 -> RMSNorm -> output layer 

The output layer is a linear layer without bias from dim (4096) to vocab_size (32000).





