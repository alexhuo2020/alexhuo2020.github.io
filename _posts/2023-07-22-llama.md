
# The model structure of LLaMA 2

Recently Meta makes the LLaMA 2 model public available. Here I will run the model and see the structure of the model.

To run the model we need to first install torch and torchrun. While I encounter error "failed to load torch._c...", the following code does not produce error:
```
python -m torch.distributed.run .py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model     --max_seq_len 512 --max_batch_
size 4
```
Now let's print the model by adding 
```
print(generator.model)
```
to the "example_chat_completion.py" file. The result is


Transformer(
  (tok_embeddings): ParallelEmbedding()
  (layers): ModuleList(
    (0-31): 32 x TransformerBlock(
      (attention): Attention(
        (wq): ColumnParallelLinear()
        (wk): ColumnParallelLinear()
        (wv): ColumnParallelLinear()
        (wo): RowParallelLinear()
      )
      (feed_forward): FeedForward(
        (w1): ColumnParallelLinear()
        (w2): RowParallelLinear()
        (w3): ColumnParallelLinear()
      )
      (attention_norm): RMSNorm()
      (ffn_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): ColumnParallelLinear()
)
