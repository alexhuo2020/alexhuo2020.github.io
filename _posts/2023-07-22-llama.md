
# The model structure of LLaMA 2

Recently Meta makes the LLaMA 2 model public available. Here I will run the model and see the structure of the model.

To run the model we need to first install torch and torchrun. While I encounter error "failed to load torch._c...", the following code does not produce error:
```
python -m torch.distributed.run .py     --ckpt_dir llama-2-7b-chat/     --tokenizer_path tokenizer.model     --max_seq_len 512 --max_batch_
size 4
```
Now let's print the model by adding 
```
print(generator.model)
```
to the "example_chat_completion.py" file. The result is

```
Transformer(
  (tok_embeddings): ParallelEmbedding()
  (layers): ModuleList(
    (0-31): 32 x TransformerBlock(
      (attention): Attention(
        (wq): ColumnParallelLinear()
        (wk): ColumnParallelLinear()
        (wv): ColumnParallelLinear()
        (wo): RowParallelLinear()
      )
      (feed_forward): FeedForward(
        (w1): ColumnParallelLinear()
        (w2): RowParallelLinear()
        (w3): ColumnParallelLinear()
      )
      (attention_norm): RMSNorm()
      (ffn_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): ColumnParallelLinear()
)
```
Next let's use the summary function from torchinfo package to show the details.
```
from torchinfo import summary
print(summary(generator.model))
```
Thre result is 
```
===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
Transformer                                        --
├─ParallelEmbedding: 1-1                           131,072,000
├─ModuleList: 1-2                                  --
│    └─TransformerBlock: 2-1                       --
│    │    └─Attention: 3-1                         67,108,864
│    │    └─FeedForward: 3-2                       135,266,304
│    │    └─RMSNorm: 3-3                           4,096
│    │    └─RMSNorm: 3-4                           4,096
│    └─TransformerBlock: 2-2                       --
│    │    └─Attention: 3-5                         67,108,864
│    │    └─FeedForward: 3-6                       135,266,304
│    │    └─RMSNorm: 3-7                           4,096
│    │    └─RMSNorm: 3-8                           4,096
│    └─TransformerBlock: 2-3                       --
│    │    └─Attention: 3-9                         67,108,864
│    │    └─FeedForward: 3-10                      135,266,304
│    │    └─RMSNorm: 3-11                          4,096
│    │    └─RMSNorm: 3-12                          4,096
│    └─TransformerBlock: 2-4                       --
│    │    └─Attention: 3-13                        67,108,864
│    │    └─FeedForward: 3-14                      135,266,304
│    │    └─RMSNorm: 3-15                          4,096
│    │    └─RMSNorm: 3-16                          4,096
│    └─TransformerBlock: 2-5                       --
│    │    └─Attention: 3-17                        67,108,864
│    │    └─FeedForward: 3-18                      135,266,304
│    │    └─RMSNorm: 3-19                          4,096
│    │    └─RMSNorm: 3-20                          4,096
│    └─TransformerBlock: 2-6                       --
│    │    └─Attention: 3-21                        67,108,864
│    │    └─FeedForward: 3-22                      135,266,304
│    │    └─RMSNorm: 3-23                          4,096
│    │    └─RMSNorm: 3-24                          4,096
│    └─TransformerBlock: 2-7                       --
│    │    └─Attention: 3-25                        67,108,864
│    │    └─FeedForward: 3-26                      135,266,304
│    │    └─RMSNorm: 3-27                          4,096
│    │    └─RMSNorm: 3-28                          4,096
│    └─TransformerBlock: 2-8                       --
│    │    └─Attention: 3-29                        67,108,864
│    │    └─FeedForward: 3-30                      135,266,304
│    │    └─RMSNorm: 3-31                          4,096
│    │    └─RMSNorm: 3-32                          4,096
│    └─TransformerBlock: 2-9                       --
│    │    └─Attention: 3-33                        67,108,864
│    │    └─FeedForward: 3-34                      135,266,304
│    │    └─RMSNorm: 3-35                          4,096
│    │    └─RMSNorm: 3-36                          4,096
│    └─TransformerBlock: 2-10                      --
│    │    └─Attention: 3-37                        67,108,864
│    │    └─FeedForward: 3-38                      135,266,304
│    │    └─RMSNorm: 3-39                          4,096
│    │    └─RMSNorm: 3-40                          4,096
│    └─TransformerBlock: 2-11                      --
│    │    └─Attention: 3-41                        67,108,864
│    │    └─FeedForward: 3-42                      135,266,304
│    │    └─RMSNorm: 3-43                          4,096
│    │    └─RMSNorm: 3-44                          4,096
│    └─TransformerBlock: 2-12                      --
│    │    └─Attention: 3-45                        67,108,864
│    │    └─FeedForward: 3-46                      135,266,304
│    │    └─RMSNorm: 3-47                          4,096
│    │    └─RMSNorm: 3-48                          4,096
│    └─TransformerBlock: 2-13                      --
│    │    └─Attention: 3-49                        67,108,864
│    │    └─FeedForward: 3-50                      135,266,304
│    │    └─RMSNorm: 3-51                          4,096
│    │    └─RMSNorm: 3-52                          4,096
│    └─TransformerBlock: 2-14                      --
│    │    └─Attention: 3-53                        67,108,864
│    │    └─FeedForward: 3-54                      135,266,304
│    │    └─RMSNorm: 3-55                          4,096
│    │    └─RMSNorm: 3-56                          4,096
│    └─TransformerBlock: 2-15                      --
│    │    └─Attention: 3-57                        67,108,864
│    │    └─FeedForward: 3-58                      135,266,304
│    │    └─RMSNorm: 3-59                          4,096
│    │    └─RMSNorm: 3-60                          4,096
│    └─TransformerBlock: 2-16                      --
│    │    └─Attention: 3-61                        67,108,864
│    │    └─FeedForward: 3-62                      135,266,304
│    │    └─RMSNorm: 3-63                          4,096
│    │    └─RMSNorm: 3-64                          4,096
│    └─TransformerBlock: 2-17                      --
│    │    └─Attention: 3-65                        67,108,864
│    │    └─FeedForward: 3-66                      135,266,304
│    │    └─RMSNorm: 3-67                          4,096
│    │    └─RMSNorm: 3-68                          4,096
│    └─TransformerBlock: 2-18                      --
│    │    └─Attention: 3-69                        67,108,864
│    │    └─FeedForward: 3-70                      135,266,304
│    │    └─RMSNorm: 3-71                          4,096
│    │    └─RMSNorm: 3-72                          4,096
│    └─TransformerBlock: 2-19                      --
│    │    └─Attention: 3-73                        67,108,864
│    │    └─FeedForward: 3-74                      135,266,304
│    │    └─RMSNorm: 3-75                          4,096
│    │    └─RMSNorm: 3-76                          4,096
│    └─TransformerBlock: 2-20                      --
│    │    └─Attention: 3-77                        67,108,864
│    │    └─FeedForward: 3-78                      135,266,304
│    │    └─RMSNorm: 3-79                          4,096
│    │    └─RMSNorm: 3-80                          4,096
│    └─TransformerBlock: 2-21                      --
│    │    └─Attention: 3-81                        67,108,864
│    │    └─FeedForward: 3-82                      135,266,304
│    │    └─RMSNorm: 3-83                          4,096
│    │    └─RMSNorm: 3-84                          4,096
│    └─TransformerBlock: 2-22                      --
│    │    └─Attention: 3-85                        67,108,864
│    │    └─FeedForward: 3-86                      135,266,304
│    │    └─RMSNorm: 3-87                          4,096
│    │    └─RMSNorm: 3-88                          4,096
│    └─TransformerBlock: 2-23                      --
│    │    └─Attention: 3-89                        67,108,864
│    │    └─FeedForward: 3-90                      135,266,304
│    │    └─RMSNorm: 3-91                          4,096
│    │    └─RMSNorm: 3-92                          4,096
│    └─TransformerBlock: 2-24                      --
│    │    └─Attention: 3-93                        67,108,864
│    │    └─FeedForward: 3-94                      135,266,304
│    │    └─RMSNorm: 3-95                          4,096
│    │    └─RMSNorm: 3-96                          4,096
│    └─TransformerBlock: 2-25                      --
│    │    └─Attention: 3-97                        67,108,864
│    │    └─FeedForward: 3-98                      135,266,304
│    │    └─RMSNorm: 3-99                          4,096
│    │    └─RMSNorm: 3-100                         4,096
│    └─TransformerBlock: 2-26                      --
│    │    └─Attention: 3-101                       67,108,864
│    │    └─FeedForward: 3-102                     135,266,304
│    │    └─RMSNorm: 3-103                         4,096
│    │    └─RMSNorm: 3-104                         4,096
│    └─TransformerBlock: 2-27                      --
│    │    └─Attention: 3-105                       67,108,864
│    │    └─FeedForward: 3-106                     135,266,304
│    │    └─RMSNorm: 3-107                         4,096
│    │    └─RMSNorm: 3-108                         4,096
│    └─TransformerBlock: 2-28                      --
│    │    └─Attention: 3-109                       67,108,864
│    │    └─FeedForward: 3-110                     135,266,304
│    │    └─RMSNorm: 3-111                         4,096
│    │    └─RMSNorm: 3-112                         4,096
│    └─TransformerBlock: 2-29                      --
│    │    └─Attention: 3-113                       67,108,864
│    │    └─FeedForward: 3-114                     135,266,304
│    │    └─RMSNorm: 3-115                         4,096
│    │    └─RMSNorm: 3-116                         4,096
│    └─TransformerBlock: 2-30                      --
│    │    └─Attention: 3-117                       67,108,864
│    │    └─FeedForward: 3-118                     135,266,304
│    │    └─RMSNorm: 3-119                         4,096
│    │    └─RMSNorm: 3-120                         4,096
│    └─TransformerBlock: 2-31                      --
│    │    └─Attention: 3-121                       67,108,864
│    │    └─FeedForward: 3-122                     135,266,304
│    │    └─RMSNorm: 3-123                         4,096
│    │    └─RMSNorm: 3-124                         4,096
│    └─TransformerBlock: 2-32                      --
│    │    └─Attention: 3-125                       67,108,864
│    │    └─FeedForward: 3-126                     135,266,304
│    │    └─RMSNorm: 3-127                         4,096
│    │    └─RMSNorm: 3-128                         4,096
├─RMSNorm: 1-3                                     4,096
├─ColumnParallelLinear: 1-4                        131,072,000
===========================================================================
Total params: 6,738,415,616
Trainable params: 6,738,415,616
Non-trainable params: 0
===========================================================================
```
So the 7B model has  6,738,415,616 parameters, i.e. ~ 7 billion parameters. 

The model looks quite simply, just 32 transformer blocks. 




